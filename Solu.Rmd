---
title: "Solu"
output:
  word_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```



## A

The overall question you will be trying to answer in this final exam is: __Why is there so much negativity on Facebook comments about politics?__
To answer this question, I will share with you a dataset that contains public Facebook data that corresponds to all the posts by Members of the U.S. Congress between January 1st, 2015 and December 31st, 2016, as well as all the comments and reactions to these posts. In addition, you will also have a dataset with sentiment predictions for each comment (negative, neutral, positive).

As a first step, you will have to clean the data and convert it to a format that can facilitate the subsequent analysis. I recommend you use a SQLite database, but you can also work with regular data frames if you prefer.

You have access to five data files. Read the text below for important information regarding their content, as well as links to download the files:

1 - [`congress-list.csv`](https://www.dropbox.com/s/p8cijczxugejcnp/congress-list.csv?dl=0) contains information about each Member of Congress, including gender, type (House representative or Senator), party (Democrat, Republican, Independent), `nominate_dim1` (an estimate of political ideology, from -1 very liberal to +1 very conservative), state and district.

IMPORTANT: this file also contains two important variables to merge all the different datasets. `bioguide_id` is the main key used to merge with external sources. `facebook` is the Facebook ID for each Member of Congress, and you should use this key to merge with the rest of the internal data sources. All files in the remaining datasets here contain this ID in the file name.

2 - [`facebook-114-posts.zip`](https://www.dropbox.com/s/trznn23wtotnkon/facebook-114-posts.zip?dl=0) contains multiple .csv files with information about each post of the legislators' pages. All variables should be self-explanatory. Remember that you shouldn't use `from_id` or `from_name` to merge across different data sources. `id` is the unique numeric ID for each post.

3 - [`facebook-114-comments.zip`](https://www.dropbox.com/s/vu2po7a35tqs3fg/facebook-114-comments.zip?dl=0) contains multiple .csv files with information about each comment on the legislators' pages. Each file corresponds to a different page. `from_id` and `from_name` here correspond to the person who wrote the comment. `likes_count` is the number of likes on each comment. `comments_count` is the number of replies to each comment. `id` is the unique numeric ID for each comment. `post_id` is the ID of the post to which this comment is replying (i.e. `id` in the posts .csv files). `is_reply` indicates whether the comment is a top-level comment (FALSE) or is a reply to an existing comment (TRUE); and if so, `in_reply_to_id` indicates the ID of the comment to which this comment is replying.

Some additional information: remember that Facebook comments have a threaded structure: whenever you write a comment, you can comment directly on the post (top-level comment) or as a reply to an existing comment (reply).

4 - [`facebook-114-reactions-totals.zip`](https://www.dropbox.com/s/yy3ams7szs3fa73/facebook-114-reactions-totals.zip?dl=0) offers statistics on the total of reactions (love, haha, angry...) to each post. `id` here corresponds to `id` in the `facebook-114-posts` datasets.

5 - [`facebook-114-comments-sentiment.zip`](https://www.dropbox.com/s/iovfv0l2wj2j5dp/facebook-114-comments-sentiment.zip?dl=0) contains datasets that predict the sentiment of each comment in the `facebook-114-comments.zip` files. There are three variables measuring the probability that each comment is negative, neutral or positive. They add up to one. You can either use the probabilities or, for each comment, predict a category based on which probability is highest.


**NOTE:** as you work on cleaning the dataset, if anything is not clear, you can ask in the forum for clarification.

1. Before you start cleaning the data, first consider how to design the database. Read the rest of the final exam to help you think through the options. How many tables should you have, and why? Clue: the answer is not five!

Table 1: information of the Members of Congress.

Table 2: information of posts.

Table 3: information of comments

```{r}


```

2. Do any required steps necessary to clean and merge the data; and then enter the datasets into a SQLite database, or into data frames that you can save to disk.

fbco eof
Make sure you do this in an efficient way. Pay special attention to variables that you will *not* need, and drop them from the tables/data.frames to save memory and be more efficient.

```{r }
library(data.table)
library(plyr)
library(dplyr)

#table 1
table1 <- congress <- fread("./data/congress-list.csv")
table1 <- fread("./data/congress-list.csv",stringsAsFactors = T)
#table 2
id <- tools::file_path_sans_ext(list.files("./data/facebook-114-posts/"))
id <- id[-which(id=="CongressmanCulberson")]
id <- id[-which(id=="CongressmanJimBridenstine")]
id <- id[-which(id=="RepCarolynMaloney")]
id <- id[-which(id=="RepChrisCollins")]

id <- id[-which(id=="HurdOnTheHill")]
id <- id[-which(id=="RepRobBishop")]

post <- list()

for(k in 1:length(id)){
  x <- id[k]
  post.path <- paste("./data/facebook-114-posts/", x, ".csv", sep="")
post.react.path <- paste("./data/facebook-114-reactions-totals/", x, "_reactions.csv", sep="")
post11 <- fread(post.path,
                colClasses = c("character", "character", "NULL", "character", "NULL","NULL", "character", "NULL", rep("integer",3)),
                select = c(1,2,4,7,9,10,11))
post11[ ,c( "likes_count2", "love_count",  "haha_count",  "wow_count",   "sad_count",   "angry_count")] <- 0
post11[ ,c( "likes_count2", "love_count",  "haha_count",  "wow_count",   "sad_count",   "angry_count")] <- NA

names(post11) <- c("member.id", "name", "created.time", "post.id", "likes_count0", "comments_count", "shares_count", 
                   "likes_count", "love_count",  "haha_count",  "wow_count",   "sad_count",   "angry_count")

react1 <- read.csv(post.react.path, stringsAsFactors = F)
for(i in 1:nrow(post11)){
  if(length(which(react1$id==post11$post.id[i])) != 0){
    post11[i,8:13] <- as.vector(react1[which(react1$id==post11$post.id[i]),2:7])
    }
}
post[[k]] <- post11
}

table2 <- rbind.fill(post)

# table 3
id <- tools::file_path_sans_ext(list.files("./data/facebook-114-comments/"))

# comment11 <- fread("./data/facebook-114-comments/7259193379_comments.csv",
#                    colClasses = c("NULL", "NULL", "NULL", "character", "integer","integer", "character", "character", rep("NULL",2)),
#                    select = c(4:8))
# comment11[,c("neg_sentiment",	"neu_sentiment",	"pos_sentiment")] <- 0
# comment11[,c("neg_sentiment",	"neu_sentiment",	"pos_sentiment")] <- NA
# names(comment11) <- c("comment.time", "comment.likes", "comments.count", 
#                       "comment.id", "post.id", "neg_sentiment", "neu_sentiment", "pos_sentiment" )
# 
# sent <- read.csv("./data/facebook-114-comments-sentiment/7259193379_comments.csv")
# for(i in 1:nrow(comment11)){
#   if(length(which(sent$id==comment11$comment.id[i])) != 0){
#     comment11[i,6:8] <- as.vector(sent[which(sent$id==comment11$comment.id[i]),2:4])
#     }
# }
comment <- list()
for(k in 1:length(id)){
  x <- id[k]
  comment.path <- paste("./data/facebook-114-comments/", x, ".csv", sep="")
sent.react.path <- paste("./data/facebook-114-comments-sentiment/", x, ".csv", sep="")
comment11 <- fread(comment.path,
                   colClasses = c("NULL", "NULL", "NULL", "character", "integer","integer", "character", "character", rep("NULL",2)),
                   select = c(4:8))
names(comment11) <- c("comment.time", "comment.likes", "comments.count",
                      "comment.id", "post.id")
#comment11[,c("neg_sentiment",	"neu_sentiment",	"pos_sentiment")] <- 0
#comment11[,c("neg_sentiment",	"neu_sentiment",	"pos_sentiment")] <- NA
#names(comment11) <- c("comment.time", "comment.likes", "comments.count",
#                      "comment.id", "post.id", "neg_sentiment", "neu_sentiment", "pos_sentiment" )

sent <- read.csv(sent.react.path,stringsAsFactors = F)
names(sent) <- c("comment.id","neg_sentiment", "neu_sentiment", "pos_sentiment")
comment[[k]] <- merge(comment11, sent, by = "comment.id",
      all= T)

}


table3 <- rbind.fill(comment)
```

3. Compute relevant summary statistics for your tables. You should **at least** answer the following questions: how many rows do you have in each table? what are the average values of all numeric variables? what are the distribution of the categorical variables? 

table 1: 532   $times$  8
table 1: 436168   $times$  13
table 1: 10603905   $times$  8.

```{r}
dim(table1)
dim(table2)
dim(table3)


table1$nominate_dim1 <- as.factor(table1$nominate_dim1)
table1$district_code <- as.factor(table1$district_code)
summary(table1)

summary(table2)

summary(table3)
```


## B
The goal of this second part of the assignment is to analyze the datasets you just created in order to answer a set of descriptive questions. Your answer to the questions will offer important context towards the overall research question:  __Why is there so much negativity on Facebook comments about politics?__

For each item below, you should write code with any statistical or graphical analysis that you consider appropriate, and then answer the question.

1. First of all, how much negativity is there on the comments of pages by U.S. legislators? In other words, what proportion of comments are negative?

predict a category based on which probability is highest
```{r }
sum(table3$neg_sentiment==1,na.rm = T)/length(na.omit(table3$neg_sentiment))
```

The proportion of comments are negative is 18.9\%.

2. How much variation is there in the level of negativity that legislators see on their Facebook pages? 
Which are the legislators with the highest and lowest proportion of negative comments?

```{r}
member <- unique(table2$member.id)
porp.neg <- NULL
for(i in 1:length(member)){
posts.temp <- unique(table2[table2$member.id==member[i],4])
comment.temp <- table3[table3$post.id%in%posts.temp,]
porp.neg[i] <- sum(comment.temp$neg_sentiment==1,na.rm = T)/length(na.omit(comment.temp$neg_sentiment))
}


max(porp.neg,na.rm = T)
min(porp.neg,na.rm = T)
member[which.max(porp.neg)]
member[which.min(porp.neg)]

table2[table2$member.id==member[which.max(porp.neg)],]
table2[table2$member.id==member[which.min(porp.neg)],]
```

max: Senator Kelly Ayotte, 42.84\%
min: Gregorio Kilili Camacho Sablan, 1.01\%

3. How did negativity evolve over time during the period of analysis? Do you identify any particular days or periods during which negativity spiked? Can you explain why?

```{r}

date <- strsplit(table3$comment.time,"T")
date <- lapply(date, function(x){x[1]})
date <- unlist(date)
table3.time <- table3[order(as.Date(date, format="%Y-%m-%d")),]

rle.ind <- rle(substr(table3.time$comment.time, 1, 7))$lengths
rle.ind <- cumsum(c(1,rle.ind))
prop.neg <- NULL
for(i in 1:(length(rle.ind)-1)){
prop.neg[i] <- sum(table3.time$neg_sentiment[rle.ind[i]:(rle.ind[i+1]-1)]==1,na.rm = T)/length(na.omit(table3.time$neg_sentiment[rle.ind[i]:(rle.ind[i+1]-1)]))
}

plot(prop.neg[-1],type = "l",xaxt="n",xlab="Time", ylab = "Negative rate")
axis(1, at=1:71,labels = unique(substr(table3.time$comment.time, 1, 7))[-1],las=2,cex.axis=0.5)
```

The negative rate hits the maximum at the beginning of the 113th United States Congress.

4. Are there any other variables in the dataset that could help you measure negativity? If so, do you find similar results to questions 2 and 3 when you use that other signal?

The ratio of sad or angry counts on like counts for each post could help measure the negativity.

```{r}
member <- unique(table2$member.id)
sad <- angry <- NULL

for(i in 1:length(member)){
posts.temp <- unique(table2[table2$member.id==member[i],4])
comment.temp <- table2[table2$post.id%in%posts.temp,]
sad[i] <- sum(comment.temp$sad_count,na.rm = T)/sum(comment.temp$likes_count,na.rm = T)
angry[i] <- sum(comment.temp$angry_count,na.rm = T)/sum(comment.temp$likes_count,na.rm = T)
}


max(sad,na.rm = T)
max(angry,na.rm = T)
member[which.max(sad)]
member[which.max(angry)]

table2[table2$member.id==member[which.max(sad)],]
table2[table2$member.id==member[which.max(angry)],]

porp.neg[which(member == "95696782238")]
```

Congressman Alcee L. Hastings, whose negativity is 13.32\%, reveives the largest ratios of sad and angry counts on the like counts.

5. (Note: this question is hard!) What proportion of comment threads have at least one negative comment?

```{r}
sum(table3$neg_sentiment>0,na.rm = T)/length(na.omit(table3$neg_sentiment))
```

73.38\% of comment threads have at least one negative comment.

## C

Now you will collect additional data to continue exploring the broader research question in the exam.

1. The website [EveryPolitician](https://everypolitician.org/) contains information on legislators around the world. Using the webscraping tools you learned in the course, create a dataset with two variables -- `bioguide_id` and `age` -- by scraping the data available in these two pages: https://everypolitician.org/united-states-of-america/house/term-table/114.html and https://everypolitician.org/united-states-of-america/senate/term-table/114.html

If you are having trouble scraping it, you can also just click on "Download data" (but you will not get full mark if you do that!)

```{r}
library('rvest')


webpage <- read_html("https://everypolitician.org/united-states-of-america/senate/term-table/114.html")

web.info <- sapply(strsplit(html_text(webpage),"\n"), function(x){
  gsub("[[:space:]]", "", x)
})

web.info <- web.info[!web.info==""]
web.info <- web.info[-c(1:79)]
name <- web.info[which(web.info=="Gender")-2]
bioguide <- web.info[which(web.info=="bioguide")+1]
birth  <- web.info[which(web.info=="Born")+1]

birth <- as.Date(birth)
ages <- 2019-year(birth)
table4 <- cbind(bioguide,ages)

```

2. Are there more negative comments on the pages of younger politicians? Use any statistical or graphical methods that you consider appropriate to answer this question.

```{r}
name.temp <- congress[congress$bioguide_id %in% bioguide,5]
name.temp <- name.temp[-c(10,50,71,81,84)] #senatorbennet, SenatorKirk,SenatorGaryPeters,chuckschumer,jeffsessions
name.temp=="jeffsessions"
neg.rate <- NULL

for (i in 1:100){
  if(!is.na(name.temp[i])){
    temp <- read.csv(paste("./data/facebook-114-comments-sentiment/", name.temp[i], "_comments.csv",sep = ""))
    neg.rate[i] <- sum(temp[,2]==1,na.rm = T)/length(na.omit(temp[,2]))
  }
}

lm.age <- ages[-c(10,50,71,81,84)]
lm.neg <- neg.rate

ind <- !is.na(neg.rate)
lm.age <- lm.age[ind]
lm.neg <- lm.neg[ind]

fit1 <- lm(lm.neg~lm.age)
summary(fit1)

plot(lm.age,lm.neg)
abline(fit1)
```

According to the hypothesis test for the coefficient of a linear model, and the graphical method, there are NO more negative comments on the pages of younger politicians.


3. The file `congress-list.csv` contained five other legislator-level variables (chamber, gender, party, ideology, state). Choose TWO of these variables and explore whether they are related with the extent to which Members of Congress receive negative comments on their Facebook pages. Write a summary of your findings.


```{r}
gender.temp <- (congress[congress$bioguide_id %in% bioguide,2])
gender.temp <- gender.temp[-c(10,50,71,81,84)] #senatorbennet, SenatorKirk,SenatorGaryPeters,chuckschumer,jeffsessions

party.temp <- (congress[congress$bioguide_id %in% bioguide,4])
party.temp <- party.temp[-c(10,50,71,81,84)] #senatorbennet, SenatorKirk,SenatorGaryPeters,chuckschumer,jeffsessions


gender.temp <- gender.temp[ind]
party.temp <- party.temp[ind]

gender.temp <- as.numeric(gender.temp=="M")
party.temp <- as.numeric(party.temp == "Republican")


fit2 <- lm(lm.neg~gender.temp+party.temp)
summary(fit2)


```

The party of sognificant for the negativity while the gender is not, statistcially speaking.

## D

To conclude this assignment, you will offer preliminary evidence regarding one potential explanation about why there is so much negativity on Facebook comments: **negative comments are widespread because they receive more engagement.** In other words, maybe negative comments generated the type of reactions on people that make them more likely to like those comments or to reply to those comments.

1. Do negative comments receive more likes than neutral or positive comments? Use any statistical or graphical methods that you consider appropriate to answer this question.

```{r}
df.temp <- na.omit(cbind(table3$comment.likes,table3$neg_sentiment))
fit3 <- lm(df.temp[,1]~df.temp[,2])
summary(fit3)

t.test(table3$comment.likes[table3$neg_sentiment>0],table3$comment.likes[table3$neg_sentiment==0])
```

It is statistically significant that negative comments receive more likes than neutral or positive comments.

2. Replicate the analysis above, but this time separately for Republicans and Democrats. Do you find any differences?

```{r}
Rep.ind <- read.csv("./RepublicansPost.csv",stringsAsFactors = F)[,2]
Rep.ind <- table3$post.id%in%Rep.ind

table3.rep <- table3[Rep.ind,]
t.test(table3.rep$comment.likes[table3.rep$neg_sentiment>0],table3.rep$comment.likes[table3.rep$neg_sentiment==0])

table3.demo <- table3[!Rep.ind,]
t.test(table3.demo$comment.likes[table3.demo$neg_sentiment>0],table3.demo$comment.likes[table3.demo$neg_sentiment==0])
```

The same conclusion obtains by the separate test for Republicans and Democrats, respectively.


3. (Note: this is a hard question) Do negative comments receive more negative replies than positive comments? 

```{r}
#set 100 likes as the threshold
t.test(table3[table3$comment.likes<=100,"neg_sentiment"],table3[table3$comment.likes<=100,"pos_sentiment"])
```

Yes, the negative comments receive more negative replies than positive comments.